{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164a8ae3",
   "metadata": {},
   "source": [
    "\n",
    "## Data Pre-Processing\n",
    "\n",
    " ### Import Packages and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0140ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25480, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.pandas.set_option(\"display.max_columns\", None)\n",
    "# Create Dataframe\n",
    "df = pd.read_csv(r\"Visadataset.csv\")\n",
    "# Print shape of dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c189c",
   "metadata": {},
   "source": [
    "\n",
    "## Data Cleaning\n",
    "    Handling Missing values\n",
    "    Handling Missing values\n",
    "    Handling Duplicates\n",
    "    Check data type\n",
    "    Understand the dataset\n",
    "Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e51669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##these are the features with nan value\n",
    "features_with_na=[features for features in df.columns if df[features].isnull().sum()>=1]\n",
    "for feature in features_with_na:\n",
    "    print(feature,np.round(df[feature].isnull().mean()*100,5), '% missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403c42d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_with_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993fb0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('case_id', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ef01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing date class from datetime module\n",
    "from datetime import date\n",
    "  \n",
    "# creating the date object of today's date\n",
    "todays_date = date.today()\n",
    "current_year= todays_date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company_age'] = current_year-df['yr_of_estab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0418f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop('yr_of_estab', inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "print('Num of Numerical Features :', len(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "print('Num of Categorical Features :', len(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "discrete_features=[feature for feature in num_features if len(df[feature].unique())<=25]\n",
    "print('Num of Discrete Features :',len(discrete_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "continuous_features=[feature for feature in num_features if feature not in discrete_features]\n",
    "print('Num of Continuous Features :',len(continuous_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14283683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('case_status', axis=1)\n",
    "y = df['case_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d566b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75067428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the target column has Denied it is encoded as 1 others as 0\n",
    "y= np.where(y=='Denied', 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac52c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of data before scaling\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, col in enumerate(['no_of_employees','prevailing_wage','company_age']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(x=X[col], color='indianred')\n",
    "    plt.xlabel(col)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39a858",
   "metadata": {},
   "source": [
    "No of employees and Copmany age column is skewed\n",
    "Apply a power transform featurewise to make data more Gaussian-like.\n",
    "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.\n",
    "\n",
    "Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform.\n",
    "\n",
    "Checking Skewness\n",
    "\n",
    "What is Skewness ?\n",
    "\n",
    "Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. A normal distribution has a skew of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check Skewness\n",
    "X[continuous_features].skew(axis=0, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a53c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "transform_features = ['company_age', 'no_of_employees']\n",
    "X_copy = pt.fit_transform(X[transform_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105898f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_copy = pd.DataFrame(X_copy, columns=transform_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "for i, col in enumerate(transform_features):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    sns.histplot(x=X_copy[col], color='indianred')\n",
    "    plt.xlabel(col)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking Skewness\n",
    "X_copy.skew(axis=0, skipna=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ba49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for feature in cat_features:\n",
    "    print(feature,':', df[feature].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73982d43",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Encoding and Scaling\n",
    "#### One Hot Encoding for Columns which had lesser unique values and not ordinal\n",
    "\n",
    "    One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
    "    Ordinal Encoding for Columns which has many unique categories\n",
    "\n",
    "#### Ordinal encoding is used here as label encoder is supported for column transformer.\n",
    "    Ordinal encoding is used for Ordinal Variable. Variable comprises a finite set of discrete values with a ranked ordering between values.\n",
    "\n",
    "### Standard Scaler\n",
    "\n",
    "    Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "### Power Transformer\n",
    "\n",
    "    Power transforms are a technique for transforming numerical input or output variables to have a Gaussian or more-Gaussian-like probability distribution.\n",
    "    Selecting number features for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeea6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = list(X.select_dtypes(exclude=\"object\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32a27d",
   "metadata": {},
   "source": [
    "### Preprocessing using Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753758a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create Column Transformer with 3 types of transformers\n",
    "or_columns = ['has_job_experience','requires_job_training','full_time_position','education_of_employee']\n",
    "oh_columns = ['continent','unit_of_wage','region_of_employment']\n",
    "transform_columns= ['no_of_employees','company_age']\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,OrdinalEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "transform_pipe = Pipeline(steps=[\n",
    "    ('transformer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, oh_columns),\n",
    "        (\"Ordinal_Encoder\", ordinal_encoder, or_columns),\n",
    "        (\"Transformer\", transform_pipe, transform_columns),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad12e9c",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTEENN(random_state=42,sampling_strategy='minority' )\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f71f6f",
   "metadata": {},
   "source": [
    "\n",
    "## Train Test Split\n",
    "    The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.\n",
    "\n",
    "    It is a fast and easy procedure to perform, the results of which allow you to compare the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00706be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import  train_test_split\n",
    "# separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res,test_size=0.2,random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162153d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \\\n",
    "                            precision_score, recall_score, f1_score, roc_auc_score,roc_curve \n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ecb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(true, predicted):\n",
    "    acc = accuracy_score(true, predicted) # Calculate Accuracy\n",
    "    f1 = f1_score(true, predicted) # Calculate F1-score\n",
    "    precision = precision_score(true, predicted) # Calculate Precision\n",
    "    recall = recall_score(true, predicted)  # Calculate Recall\n",
    "    roc_auc = roc_auc_score(true, predicted) #Calculate Roc\n",
    "    return acc, f1 , precision, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d995e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "     \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"XGBClassifier\": XGBClassifier(), \n",
    "     \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "     \"Support Vector Classifier\": SVC(),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier()\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which can evaluate models and return a report \n",
    "def evaluate_models(X, y, models):\n",
    "    '''\n",
    "    This function takes in X and y and models dictionary as input\n",
    "    It splits the data into Train Test split\n",
    "    Iterates through the given model dictionary and evaluates the metrics\n",
    "    Returns: Dataframe which contains report of all models metrics with cost\n",
    "    '''\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    models_list = []\n",
    "    accuracy_list = []\n",
    "    auc= []\n",
    "    \n",
    "    for i in range(len(list(models))):\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, y_train) # Train model\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Training set performance\n",
    "        model_train_accuracy, model_train_f1,model_train_precision,\\\n",
    "        model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)\n",
    "\n",
    "\n",
    "        # Test set performance\n",
    "        model_test_accuracy,model_test_f1,model_test_precision,\\\n",
    "        model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)\n",
    "\n",
    "        print(list(models.keys())[i])\n",
    "        models_list.append(list(models.keys())[i])\n",
    "\n",
    "        print('Model performance for Training set')\n",
    "        print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "        print('- F1 score: {:.4f}'.format(model_train_f1)) \n",
    "        print('- Precision: {:.4f}'.format(model_train_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_train_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
    "\n",
    "        print('----------------------------------')\n",
    "\n",
    "        print('Model performance for Test set')\n",
    "        print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
    "        accuracy_list.append(model_test_accuracy)\n",
    "        print('- F1 score: {:.4f}'.format(model_test_f1))\n",
    "        print('- Precision: {:.4f}'.format(model_test_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_test_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
    "        auc.append(model_test_rocauc_score)\n",
    "        print('='*35)\n",
    "        print('\\n')\n",
    "        \n",
    "    report=pd.DataFrame(list(zip(models_list, accuracy_list)), columns=['Model Name', 'Accuracy']).sort_values(by=['Accuracy'], ascending=False)\n",
    "        \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a96d7",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_report =evaluate_models(X=X_res, y=y_res, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db86dc2",
   "metadata": {},
   "source": [
    "\n",
    "## Results of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d00082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0641a",
   "metadata": {},
   "source": [
    "\n",
    "Here we can use Random Forest for Hyper Parameter Tuning\n",
    "\n",
    "Define the parameter distribution for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3576d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize few parameter for Hyperparamter tuning\n",
    "xgboost_params = {\n",
    "    'max_depth':range(3,10,2),\n",
    "    'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    \"max_depth\": [10, 12, None, 15, 20],\n",
    "    \"max_features\": ['sqrt', 'log2', None],\n",
    "    \"n_estimators\": [10, 50, 100, 200]\n",
    "}\n",
    "\n",
    "knn_params = {\n",
    "    \"algorithm\": ['auto', 'ball_tree', 'kd_tree','brute'],\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"n_neighbors\": [3, 4, 5, 7, 9],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ed14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models list for Hyperparameter tuning\n",
    "randomcv_models = [\n",
    "    ('XGBoost', XGBClassifier(), xgboost_params),\n",
    "    (\"RF\", RandomForestClassifier(), rf_params),\n",
    "    (\"KNN\", KNeighborsClassifier(), knn_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7598d",
   "metadata": {},
   "source": [
    "Create a function for model training and report which can be used in hyperparameter tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ad3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "for name, model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=100,\n",
    "                                   cv=3,\n",
    "                                   verbose=2, \n",
    "                                   n_jobs=-1)\n",
    "    random.fit(X_res, y_res)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"---------------- Best Params for {model_name} -------------------\")\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "best_models = {\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(**model_param['RF']),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(**model_param['KNN']),\n",
    "    \"XGBClassifier\": XGBClassifier(**model_param['XGBoost'],n_jobs=-1),\n",
    "}\n",
    "tuned_report =evaluate_models(X=X_res, y=y_res, models=best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuned_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dada7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = KNeighborsClassifier(**model_param['KNN'])\n",
    "best_model = best_model.fit(X_train,y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "cr = classification_report(y_test,y_pred)\n",
    "\n",
    "print(\"FINAL MODEL 'KNN'\")\n",
    "print (\"Accuracy Score value: {:.4f}\".format(score))\n",
    "print (cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43223e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00e761",
   "metadata": {},
   "source": [
    "\n",
    "# Best Model is K-Nearest Neighbor(KNN) with Accuracy 96.66%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
